
6/3/22

After creating a network that achieves 90% win rate on a 6x6 board, I decided to try various changes to improve the win rate. First, I tried training a policy network, but this was more difficult than imagined, and since the action space in Snake is so small, the policy network will not be much help in training the network.

I tried to standardize the learning routine by decreasing the learning rate in inverse proportion to the current maximum score. This proved an effective way to approach altering the learning rate. Through experimentation, and perhaps contrary to intuition, I found that the learning rate should decrease faster than in inverse proportion to the current maximum score.

Now, I'm trying an unfolding technique to both speed up the training process and provide a more effective tree search. This is perhaps the most promising technique so far.


6/4/22

After a perfect run on my 6x6 snake, I decided to expand to 10x10 snake. This posed many challenges. My first attempt network was to 6x10x10 (5x5) 9x10x10 (5x5) 9x5x5 -> 150 -> 1. This network does not work very effectively. Perhaps it is my function for sampling actions that is off. Or perhaps it is that my network is not large enough. I'll try to diagnose the problem first and then go from there.

After printing a game with action probabilities, it seems that the action sampling is not the biggest concern. The network design may be at the most fault here.


6/6/22

Notable game on 10x10 board: fastest way to lose

81,3,0,0,1,2,1,2,2,1,2,1,0,60,1,2,3,0,3,2,81,0,3,2,3,3,0,3,2


6/10/22

TURN OFF SANITIZER. It practically doubles run time.


6/11/22

Network I decided:
(5x5) 6x10x10 (5x5) 9x10x10 (pool) 9x5x5 (5x5) 9x5x5 -> 150 -> 1
Possible candidate:
(7x7) 10x8x8 (5x5) 10x8x8 (5x5) 15x8x8 (pool) 15x4x4 -> 200 -> 1

Note: Learning rate is extremely important, and it is best to tune this value for different networks. For the network I decided on going on, 0.002 is too large. I trained the program overnight with learning rate 0.0007 and it worked fairly well, giving an average score of 41. Now I'm trying an even smaller rate of 0.0003.

6/12/22
ALWAYS REMEMBER TO CAFFEINATE.


6/13/22

I tried training the candidate network from scratch, but it didn't work. I guess it is just hard to train networks from scratch.

Training it using 100 games played by the deterministic algorithm proved more effective.

Compiling command:
g++ main.cpp data.cpp convNet.cpp environment.cpp InputLayerCode.cpp trainer.cpp deterministicFunctions.cpp

Then:
./a.out

6/16/22
SNAIL: System for Neural Autonomous Iterative Learning


6/19/22
Game 163: first 10x10 snake solve.


6/20/22

A slight problem with the reward-based learning framework: when I remove single-action states, the states skipped between the states should be counted as extra time steps in the value formula. Should raise discountFactor to the power of the time difference between states.
